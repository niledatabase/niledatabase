export const metadata = {
  title: "Distributed DDL at Global Scale",
  authors: ["gwenshap"],
  image: "2024-11-19-distributed-ddl/cover.webp",
  sizzle:
    "TBD",
  tags: ["database", "serverless", "postgres", "distributed systems"],
};

When we need to describe Nile in a single sentence, we say "PostgreSQL re-engineered for multi-tenant apps". By multi-tenant app we mean applications like Stripe, Figma, Twilio, Notion, Workday, Gusto... where a large number of customers is served off a single application stack. In this type of application, a key part of the architecture is deciding how to store the data for each customer. 

Broadly speaking, there are two main approaches. The first is database per tenant (or sometimes schema per tenant):

![Database per tenant architecture](/blog/2024-11-19-distributed-ddl/db_per_tenant.png)

This architecture provides isolation and flexibility, but it uses more resources and more effort to operate. 

The other approach is to place all tenants in the same shared schema and add a tenant identifier to all the tables:

![Shared schema for all tenants](/blog/2024-11-19-distributed-ddl/shared_schema.png)

This approach is simple and cheap, which is why most new applications start here. Later on, it can run into scalability issues or ability to adapt to individual customer needs and requirements. 

Which is why, no matter where you start, you eventually end up with a hybrid model. In a typical hybrid architecture, the database is scaled out by sharding. Most tenants are placed on shared shards, but larger and more demanding customers get allocated dedicated shards.

Nile's model aims at providing you with the simple developer experience of shared schema, but with the isolation and flexibility of DB per tenant. In our model, tenants are separated into "virtual tenant databases". These virtual tenant databases can be placed on shared compute (which may be sharded, depending on the workload), or on dedicated compute. Regardless of how the tenenant databases are organized, developers can work as if they were all in one shared schema. 

![Nile virtual databases](/blog/2024-11-19-distributed-ddl/nile_virtual_databases.png)

Our goal is to have any number of virtual databases, spread across a number of physical postgres instances, and yet provide the developer experience of a single schema shared by all tenants. DDLs are SQL commands that modify the schema - things like `CREATE TABLE`, `ALTER TABLE`, `DROP INDEX`, etc. Our goals for the developer experience with DDLs were:

* Each DDL applies to all tenants 
* At the same time (or as close to it as feasible)
* DDL should behave exactly as they normally do in Postgres. This includes supporting transactions with DDL (IMO, one of the best features in Postgres).
* The fact that each DDL executes over multiple virtual databases and physical instances should be completely transparent to developers

Lets see how we implemented this. I'll start with a high level overview of the architecture and walk you through the green path flow of executing one simple DDL. Then I'll dive into the details of how we solved the 3 most challenging problems with distributed DDLs: Transactions, locks and failure handling. On the way, we'll share some tips and tricks we used in building our Postgres extension, which may come handy when you decide to write your own extensions.

Get some coffee and lets get started.

## Distributed DDL architecture walkthrough

We've implemented our distributed DDL with two main components:
- `nile_ddl` extension. This extension is loaded into every Postgres instance and is responsible for intercepting DDLs, extracting key information and initiating the distributed transactions.
- `transaction coordinator`. This is a stand-alone service that distributes the DDLs to all relevant databases and guarantees that each DDL is either applied successfully to all databases or to none of them.

To understand how these components work together to execute a DDL statement, lets look at what happens when a user connects to their Nile database and sends `CREATE TABLE` command. This command is sent from the client to one of the Postgres instances. 

### Intercepting DDL with `process_utility_hook`

In order to intercept the DDL before it executes, `nile_ddl` uses the `process_utility_hook`. To understand why and how our extension uses the `process_utility_hook`, we need to know what is a utility and how Postgres handles them. 

In Postgres terms, a "utility" is any command except SELECT, INSERT, UPDATE and DELETE. This includes all DDL as well as other commands like COMMIT, NOTIFY or DO. When Postgres recieves a utility command, it uses `ProcessUtility(..)` method to process it. This is a simple wrapper that looks like this:

```
	if (ProcessUtility_hook)
		(*ProcessUtility_hook) (...);
	else
		standard_ProcessUtility(...);
```

It checks if there are any extensions that want to process the utility command before Postgres runs its standard processing. `nile_ddl` extension provides such a hook, and is therefore called before Postgres processes any command. This is super useful because `process_utility_hook` is called for almost everything that isn't a query or a DML. This means that we have one entry point for almost everything we need to process. 

Once we are done processing the command, it is our responsibility to call `standard_ProcessUtility`,  so that Postgres can continue with its standard flow. A bit off-topic, but in case you are curious: `standard_ProcessUtility` method is essentially a gigantic switch statement that routes every one of the 60+ utility commands to their appropriate handler.

![Process utility hook](/blog/2024-11-19-distributed-ddl/process_utility_hook.png)

So, our extension got called with a utility command. What now? Now it has to do few things:

1. Check that the command is actually a DDL. If the command isn't `ALTER`, `CREATE` or `DROP` we are not interested and can skip it.
2. Raise error on unsupported DDL. Nile has a few restrictions on the relations we allow. For example, primary keys **must** include the `tenant_id` column and `tablespace` commands are not supported at all (Nile handles these automatically). 
3. Determine which locks the DDL requires.
4. Begin a distributed transaction (only needed if this is the first DDL in a transaction)
5. Ask the transaction coordinator to distribute the locks

The reason we extract the locks and distribute them so early in the process is in order to minimize the time we wait while holding the lock. Most DDL require `access exclusive` lock, which not only prevents any queries from accessing the table in question while the DDL is executing, it also prevents any new queries from accessing the table while the DDL is waiting to acquire the lock. This makes these locks quite expensive. To minimize the risk and the time spent while holding the lock, Nile will attempt to acquire the necessary locks, with a short lock timeout, on all relevant databases before starting to execute the DDLs. If the lock acquisition failed on any database, the DDL will return an error rather than continue waiting for the lock.

Acquiring locks with a short timeout before executing DDLs is considered a best practice in Postgres, for the reasons we just explained. Nile's distributed DDL implement this best practice for our users.

### Transaction coordinator

Once `nile_ddl` extension determines the necessary locks, it calls the transaction coordinator to start a transaction (if necessary) and distribute the locks.
Starting the transaction is very straight forward - the coordinator has an open connection to all databases, and it simply sends everyone `BEGIN`. 

Distributing the locks is similar: The coordinator sends all databases the commands that acquire the locks (more on that in the section on locking). In order to avoid deadlocks, we make sure to always acquire locks in the exact same order - both order of databases on which we acquire the locks and order of locks within each database. This guarantees that we'll fail early in case of conflict and that we won't run into a case where two concurrent transactions each wait for the other to release a lock on a different database.

![Distributing locks](/blog/2024-11-19-distributed-ddl/distribute_locks.png)

Once the locks are acquired, the `nile_ddl` extension asks the coordinator to distribute the DDL itself. While the coordinator sends all the other databases the DDL to execute, the extension where the DDLs originated can proceed to call `standard_ProcessUtility` and process the DDL locally.

Meanwhile, every remote database recieved a DDL. Since every database is running our extension, these DDLs are intercepted by the extension on each database. It is very important the extension will not attempt to distribute this DDL, as this will lead to an infinite loop. Therefore the extension has to know that these DDLs arrived from the transaction coordinator and can be sent to `standard_ProcessUtility` directly. We do this via a configuration (GUC) that is set when the transaction coordinator first initializes the transaction, this configuration tells the extension that it doesn't need to process the DDLs that follow - they were already validated by the extension that the origin database and distributed by the coordinator.

![Distributing DDL](/blog/2024-11-19-distributed-ddl/distribute_ddl.png)

Once all the databases finished processing the DDL, the coordinator returns successful completion to the extension. The extension, which meanwhile finished processing the DDL locally, can return the response to the client. We have successfully executed a distributed DDL. 

Or almost. We still need to commit the transaction. And in order to maintain atomicity guarantees, it has to commit on all databases or on none. Lets see how we do this.

## Transactions

You may remember from an earlier section that `BEGIN`, `COMMIT`, `ABORT` and `ROLLBACK` are all utility commands, so it may seem that we can handle transactions by having our `process_utility_hook` intercept and handle these commands in order to implement distributed transactions. This is a tempting path, but it has some critical drawbacks:

- Intercepting every `BEGIN` will be very costly in a transaction-heavy OLTP database. And most of this effort will be wasted. An OLTP system will process billions of DML transactions for every DDL. 
- Transactions can be implicit - stand alone DDL is a transaction for instance. We won't always have a `BEGIN` to intercept. We must handle each DDL as potentially starting a transaction.
- Intercepting a `COMMIT` only gives us a single point to intercept - when the `COMMIT` command is sent. Distributed transactions require a two-phase commit, which can't be implemented with this single hook.

Fortunately, Postgres provides a better way for extensions to hook into the transaction lifecycle - `XactCallback` (pronounced "transaction callback").

This callback is called on variable events in the transaction lifecycle with an enum parameter that specifies which event occured. The events that we are interested in are:

- `XACT_EVENT_PRE_COMMIT` - This event happens before the commit itself. The callback method is allowed to return an error while processing it. If it returns an error, Postgres will abort the transaction and force a rollback. 
- `XACT_EVENT_COMMIT` - This event happens after the commit, and the callback is not allowed to return an error when processing it. No matter what happened, it must get to a healthy and successful state since the client it going to recieve a message that the transaction was successfully committed.
- `XACT_EVENT_ABORT` - This event is called when the transaction is aborted and the callback needs to handle rollback.

`nile_ddl` extension maps these events into a distributed two phase commit.




## Locks

## When things go wrong

## Sum things up




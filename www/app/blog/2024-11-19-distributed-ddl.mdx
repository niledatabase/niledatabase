export const metadata = {
  title: "Distributed DDL at Global Scale",
  authors: ["gwenshap"],
  image: "2024-11-19-distributed-ddl/cover.webp",
  sizzle:
    "TBD",
  tags: ["database", "serverless", "postgres", "distributed systems"],
};

When we need to describe Nile in a single sentence, we say **"PostgreSQL re-engineered for multi-tenant apps"**. By multi-tenant apps, we mean applications like Stripe, Figma, Twilio, Notion, Workday, and Gusto - here a large number of customers is served from a shared application stack. In these types of applications, a key architectural challenge is deciding how to store data for each customer. 

Broadly speaking, there are two main approaches. The first is **database per tenant** (or sometimes **schema per tenant**):

![Database per tenant architecture](/blog/2024-11-19-distributed-ddl/db_per_tenant.png)

This architecture provides isolation and flexibility but requires more resources and effort to operate. 

The other approach is to place all tenants in the same shared schema and add a tenant identifier to every table:

![Shared schema for all tenants](/blog/2024-11-19-distributed-ddl/shared_schema.png)

This approach is simple and cost-effective, which is why most new applications start here. However, over time, it can run into scalability issues and difficulties in adapting to individual customer needs and requirements. 

That's why, no matter where you start, you eventually end up with a **hybrid model**. In a typical hybrid architecture, the database is scaled out through sharding. Most tenants are placed on shared shards, while larger and more demanding customers are allocated dedicated shards.

Nile's model aims to provide the simple developer experience of shared schema while maintaining the isolation and flexibility of a database-per-tenant model. In our system, tenants are separated into **"virtual tenant databases"**. These virtual tenant databases can be placed on shared compute (which may be sharded depending on the workload), or on dedicated compute. Regardless of how the tenenant databases are organized, developers can work as if everything were in a single shared schema. 

![Nile virtual databases](/blog/2024-11-19-distributed-ddl/nile_virtual_databases.png)

Our goal is to support any number of virtual databases, distributed across a number of physical PostgreSQL instances, while still providing the seamless developer experience of a single schema shared by all tenants. **DDLs** (Data Definition Language commands) are SQL commands that modify the schema - things like `CREATE TABLE`, `ALTER TABLE` and `DROP INDEX`. Our goals for the developer experience with DDLs are:

* Each DDL applies to all tenants concurrently
* Behaves exactly as it normally would in Postgres. This includes supporting transactions with DDL and all the transactional guarantees (which we consider one of PostgreSQL's best features).
* The fact that each DDL executes over multiple virtual databases and physical instances should be completely transparent to developers

Let's dive into the architecture and implementation. we'll start with a high-level overview of the architecture and walk you through the green-path flow of executing a simple DDL. Then we'll dive into the details of how we solved the three most challenging problems with distributed DDLs: transactions, locks and failure handling. Along the way, we'll share some tips and tricks we used in building our Postgres extension -- these might come in handy if you decide to write your own extensions.

So, grab a coffee, and let's get started!

## Distributed DDL architecture walkthrough

We've implemented our distributed DDL system using two main components:
1. **`nile_ddl` extension:** This extension is loaded into every Postgres instance. It is responsible for intercepting DDL statements, extracting key information, and initiating the distributed transactions.
2. **Transaction coordinator:** This is a stand-alone service that distributes the DDL statements to all relevant databases and ensures that each DDL is either applied successfully to all databases or to none of them.

To understand how these components work together to execute a DDL statement, let's look at what happens when a user connects to their Nile database and issues a  `CREATE TABLE` command. This command is sent from the client to one of the Postgres instances, where it is intercepted by the `nile_ddl` extension. 

### Intercepting DDL with `processUtility_hook`

In order to intercept the DDL before it executes, `nile_ddl` uses the `processUtility_hook`. To understand why and how our extension uses this hook, we first need to explain what a utility command is and how PostgreSQL handles them. 

In Postgres terms, a "utility" is any command except `SELECT`, `INSERT`, `UPDATE` and `DELETE`. This includes all DDL commands as well as other commands like `COMMIT`, `NOTIFY` or `DO`. When Postgres recieves a utility command, it uses `ProcessUtility(..)` method to process it. This is a simple wrapper that looks like this:

```c
	if (ProcessUtility_hook)
		(*ProcessUtility_hook) (...);
	else
		standard_ProcessUtility(...);
```

This method checks if there are any extensions that want to process the utility command before Postgres runs its standard processing. The `nile_ddl` extension provides such a hook and, as a result, is called before Postgres processes any command. This is incredibly useful because `processUtility_hook` is triggered for nearly everything that isn't a query or a DML, giving us a single entry point for almost everything we need to handle. 

Once we are finish processing the command, it is our responsibility to call `standard_ProcessUtility`,  so that Postgres can continue its normal flow. A bit off-topic, but in case you're curious: `standard_ProcessUtility` method is essentially a gigantic switch statement that routes every one of the 60+ utility commands to their appropriate handler.

![Process utility hook](/blog/2024-11-19-distributed-ddl/process_utility_hook.png)

So, our extension gets called with a utility command. What happens next? At this point, it needs to perform a few tasks:

1. **Check that the command is actually a DDL.** If the command isn't `ALTER`, `CREATE` or `DROP` we can skip it.
2. **Raise error for unsupported DDL.** Nile has specific restrictions on the type of relations we allow. For example, primary keys **must** include the `tenant_id` column, and `tablespace` commands are not supported at all (Nile handles these automatically). 
3. **Determine which locks the DDL requires.**
4. **Begin a distributed transaction** (only needed if this is the first DDL in a transaction)
5. **Ask the transaction coordinator to distribute the locks**

The reason we extract the locks and distribute them early in the process is to minimize the time spent holding a lock. Most DDL statements require an `ACCESS EXCLUSIVE` lock, which not only prevents any queries from accessing the table in question while the DDL is executing but also prevents any new queries from accessing the table while the DDL is waiting to acquire the lock. To minimize the risk and the time spent while holding the lock, Nile attempts to acquire the necessary locks - with a short lock timeout - on all relevant databases before starting to execute the DDLs. If the lock acquisition fails on any database, the DDL will return an error rather than continue waiting for the lock.

Acquiring locks with a short timeout before executing DDLs is considered a best practice in Postgres for the reasons we just explained. Nile's distributed DDL implement this best practice for our users.

### Transaction coordinator

Once the `nile_ddl` extension determines the necessary locks, it calls the transaction coordinator to start a transaction (if necessary) and distribute the locks.

Starting the transaction is straightforward: the coordinator maintains open connections to all databases and simply sends each one a `BEGIN` command. 

Distributing the locks works similarly. The coordinator sends all databases the commands required to acquire the locks (more details in the section on locking). To avoid deadlocks, we ensure that locks are always acquired in the exact same order - both in terms of the sequence of databases and the order of locks within each database. This guarantees early failure in the event of conflicts and prevents situations where two concurrent transactions are each waiting for the other to release a lock on a different database.

![Distributing locks](/blog/2024-11-19-distributed-ddl/distribute_locks.png)

Once the locks are acquired, the `nile_ddl` extension instructs the coordinator to distribute the DDL itself. While the coordinator sends the DDL command to all other databases, the original extension that first recieved the DDL proceeds to call `standard_ProcessUtility` and process the DDL locally.

Meanwhile, each remote database recieves the DDL command. Since every database runs our extension, these DDL commands are intercepted by the extension on each database. It is crucial that the extension does not attempt to redistribute these DDLs, as doing so would lead to an infinite loop. Therefore, the extension has to recognize that these DDLs were sent by the transaction coordinator and can be passed directly to `standard_ProcessUtility`. To achieve this, we use a configuration (GUC) set when the transaction coordinator initializes the transaction. This configuration indicates to the extension that it doesn't need to reprocess the DDLs that follow, as they have already been validated by the originating extension and distributed by the coordinator.

![Distributing DDL](/blog/2024-11-19-distributed-ddl/distribute_ddl.png)

Once all the databases finished processing the DDL, the coordinator notifies the originating extension of successful completion. The extension, which has already finished processing the DDL locally, can return the response to the client. At this point, we have successfully executed a distributed DDL. 

Or almost. We still need to commit the transaction. To maintain atomicity guarantees, it must commit on all databases or none at all. Let's look in detail at how we commit the distributed transaction.

## Transactions

You may recall from an earlier section that `BEGIN`, `COMMIT`, `ABORT` and `ROLLBACK` are all utility commands. So it might seem like we could handle transactions by having our `processUtility_hook` intercept these commands and implement distributed transactions. This approach is tempting, but it has some critical drawbacks:

- Intercepting every `BEGIN` will be very costly in a transaction-heavy database. Most of this effort will be wasted, as an OLTP system processes billions of DML transactions for every DDL. 
- Transactions can be implicit - standalone DDL statements are treated as transactions, for example. We won't always have a `BEGIN` to intercept, so we must treat each DDL as potentially starting a transaction.
- Intercepting a `COMMIT` only gives us a single point to intercept - when the `COMMIT` command is sent. However, distributed transactions require a two-phase commit, which can't be implemented with this single hook.

Fortunately, PostgreSQL provides a better mechanism for extensions to hook into the transaction lifecycle: `XactCallback` (pronounced "transaction callback").

This callback is triggered on various events within the transaction lifecycle, with an enum parameter that specifies which event occured. The events that we are interested in are:

- `XACT_EVENT_PRE_COMMIT` - This event occurs just before the commit itself. The callback method is allowed to return an error while processing it. If an error is returned, Postgres will abort the transaction and force a rollback. 
- `XACT_EVENT_COMMIT` - This event happens after the commit. At this point, the callback is not allowed to return an error. Regardless of what happened, it must reach a healthy and successful state since the client will recieve a confirmation that the transaction was successfully committed.
- `XACT_EVENT_ABORT` - This event is triggered when the transaction is abortedm and the callback needs to handle the rollback.

The `nile_ddl` extension maps these events into a distributed two-phase commit process.



## Locks

## When things go wrong

## Sum things up



